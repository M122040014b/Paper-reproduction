{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬股；建立滾動窗口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yfinance as yf\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from io import StringIO\n",
    "# from tqdm import tqdm  \n",
    "\n",
    "# twse_url = \"https://isin.twse.com.tw/isin/C_public.jsp?strMode=2\"\n",
    "# response = requests.get(twse_url)\n",
    "\n",
    "# # 使用 BeautifulSoup 解析 HTML\n",
    "# soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# table = soup.find('table', {'class': 'h4'})\n",
    "# rows = table.find_all('tr')\n",
    "\n",
    "# data = []\n",
    "# for row in rows[1:]:  # 忽略表格標題\n",
    "#     cols = row.find_all('td')\n",
    "#     if len(cols) >= 5:  # 確保至少有5個欄位\n",
    "#         ticker_info = cols[0].text.strip()  # 股票代號和公司名稱\n",
    "#         ticker = ticker_info.split()[0]  # 股票代號\n",
    "#         company_name = ticker_info.split()[1] if len(ticker_info.split()) > 1 else \"\"  # 公司名稱\n",
    "#         state = cols[3].text.strip()\n",
    "#         industry = cols[4].text.strip()  # 產業類別\n",
    "#         if ticker.isnumeric():  # 確保是有效的股票代號\n",
    "#             data.append([ticker, company_name, state, industry])\n",
    "#     else:\n",
    "#         print(f\"Skipping row due to insufficient columns: {cols}\")\n",
    "\n",
    "# data = pd.DataFrame(data,columns= ['Ticker','Name','State','Sector'])        \n",
    "# data['Sector'].replace(r'^\\s*$', pd.NA, regex=True, inplace=True)\n",
    "# data = data.dropna(subset=['Sector']) \n",
    "# data.head()\n",
    "# data = data[data['State'] == '上市']\n",
    "\n",
    "# data.head()\n",
    "# data.loc[:, 'Ticker'] = data['Ticker'] + '.TW'\n",
    "\n",
    "# stock = yf.download(\" \".join(data['Ticker'].values), start=\"2018-10-17\", end=\"2024-10-17\")\n",
    "# stock.index = stock.index.strftime('%Y-%m-%d')\n",
    "# stock = stock.unstack(0)\n",
    "# stock = pd.DataFrame(stock)\n",
    "# stock = stock.swaplevel('Date', 'Price')\n",
    "# stock = stock.reset_index()\n",
    "\n",
    "# stock = stock.pivot(index=['Date', 'Ticker'], \n",
    "#                     columns='Price', \n",
    "#                     values=0).reset_index()\n",
    "\n",
    "# stock = pd.merge(stock,data,on = 'Ticker')\n",
    "\n",
    "# df = stock\n",
    "# def calculate_cumulative_return(group, k = 20):\n",
    "#     group['daily_return'] = group['Adj Close'].pct_change(1).shift(-1) + 1\n",
    "#     group['y'] = group['daily_return'].apply(lambda x: 1 if x > 1.005 else 0)\n",
    "#     # group['ret20'] = group['daily_return'].rolling(window=k).apply(lambda x: x.prod(), raw=True).shift(-19) - 1\n",
    "#     # group['y'] = group['ret20'].apply(lambda x: 1 if x > 0 else 0)\n",
    "#     return group\n",
    "\n",
    "# df = df.groupby('Ticker').apply(calculate_cumulative_return).reset_index(drop = True)\n",
    "\n",
    "# df = df.dropna()\n",
    "\n",
    "# df['Date'] =  pd.to_datetime(df['Date'],format=\"%Y-%m-%d\")\n",
    "# df.set_index('Date', inplace=True)\n",
    "\n",
    "# counts_df = df.groupby('Ticker').size().reset_index(name='Counts')\n",
    "\n",
    "# different_sizes = counts_df[counts_df['Counts'] != counts_df['Counts'].iloc[0]]\n",
    "\n",
    "# tickers_to_remove = different_sizes['Ticker'].tolist()\n",
    "\n",
    "# df = df[~df['Ticker'].isin(tickers_to_remove)]\n",
    "\n",
    "# df.to_pickle(r'C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\stock.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "0    0.678052\n",
      "1    0.321948\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(r'C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\Stock.pkl')\n",
    "print(df['y'].value_counts(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder_name</th>\n",
       "      <th>trading_period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-10-16_2020-10-16</td>\n",
       "      <td>2020-10-17_2021-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-15_2021-01-15</td>\n",
       "      <td>2021-01-17_2021-04-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-04-16_2021-04-16</td>\n",
       "      <td>2021-04-17_2021-07-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-16_2021-07-16</td>\n",
       "      <td>2021-07-17_2021-10-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-15_2021-10-15</td>\n",
       "      <td>2021-10-17_2022-01-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             folder_name         trading_period\n",
       "0  2018-10-16_2020-10-16  2020-10-17_2021-01-15\n",
       "1  2019-01-15_2021-01-15  2021-01-17_2021-04-16\n",
       "2  2019-04-16_2021-04-16  2021-04-17_2021-07-16\n",
       "3  2019-07-16_2021-07-16  2021-07-17_2021-10-15\n",
       "4  2019-10-15_2021-10-15  2021-10-17_2022-01-14"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pandas.tseries.offsets import BDay\n",
    "\n",
    "# def create_time_period_folders(start_date, end_date, train_years=2, predict_months=3):\n",
    "#     start = pd.to_datetime(start_date)\n",
    "#     end = pd.to_datetime(end_date)\n",
    "\n",
    "#     time_windows = []\n",
    "#     trading_start = start\n",
    "    \n",
    "#     while trading_start <= end:\n",
    "\n",
    "#         train_end = trading_start - BDay(1)  \n",
    "#         train_start = train_end - pd.DateOffset(years=train_years)  \n",
    "#         trading_end =trading_start + pd.DateOffset(months = 3) -BDay(1)\n",
    "        \n",
    "#         time_windows.append({\n",
    "#             'folder_name': f\"{train_start.strftime('%Y-%m-%d')}_{train_end.strftime('%Y-%m-%d')}\",\n",
    "#             'trading_period': f\"{trading_start.strftime('%Y-%m-%d')}_{trading_end.strftime('%Y-%m-%d')}\"\n",
    "#         })\n",
    "        \n",
    "#         trading_start += pd.DateOffset(months=3)\n",
    "    \n",
    "#     return time_windows\n",
    "\n",
    "# start_date = \"2020-10-17\"\n",
    "# end_date = \"2024-10-17\"\n",
    "\n",
    "# time_windows = create_time_period_folders(start_date, end_date)\n",
    "# pd.DataFrame(time_windows).to_pickle(r'C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\training\\time_windows.pkl')\n",
    "# pd.DataFrame(time_windows).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed window 2018-10-16_2020-10-16 with 30 images\n",
      "Completed window 2019-01-15_2021-01-15 with 30 images\n",
      "Completed window 2019-04-16_2021-04-16 with 30 images\n",
      "Completed window 2019-07-16_2021-07-16 with 30 images\n",
      "Completed window 2019-10-15_2021-10-15 with 30 images\n",
      "Completed window 2020-01-14_2022-01-14 with 30 images\n",
      "Completed window 2020-04-15_2022-04-15 with 30 images\n",
      "Completed window 2020-07-15_2022-07-15 with 30 images\n",
      "Completed window 2020-10-14_2022-10-14 with 30 images\n",
      "Completed window 2021-01-16_2023-01-16 with 30 images\n",
      "Completed window 2021-04-14_2023-04-14 with 30 images\n",
      "Completed window 2021-07-14_2023-07-14 with 30 images\n",
      "Completed window 2021-10-16_2023-10-16 with 30 images\n",
      "Completed window 2022-01-16_2024-01-16 with 30 images\n",
      "Completed window 2022-04-16_2024-04-16 with 30 images\n",
      "Completed window 2022-07-16_2024-07-16 with 30 images\n",
      "Completed window 2022-10-16_2024-10-16 with 30 images\n"
     ]
    }
   ],
   "source": [
    "# from CNN_stock.image_to_tensor import StockDataset\n",
    "\n",
    "# seed = 123\n",
    "# np.random.seed(seed)\n",
    "\n",
    "# def generate_images_by_period(df, base_output_dir, time_windows):\n",
    "#     images_dict = {}\n",
    "#     all_jumps = {}\n",
    "    \n",
    "#     for window in time_windows:\n",
    "#         period_dir = os.path.join(base_output_dir, window['folder_name'])\n",
    "#         os.makedirs(period_dir, exist_ok=True)\n",
    "        \n",
    "#         train_data = df[\n",
    "#             (df.index >= window['train_start']) & \n",
    "#             (df.index < window['train_end'])\n",
    "#         ]\n",
    "        \n",
    "#         dates = train_data.index\n",
    "#         period_images = {}\n",
    "    \n",
    "#         i = 0\n",
    "#         jumps = []\n",
    "#         count = 0\n",
    "        \n",
    "#         while i < len(dates) - 20 and count < 30:  \n",
    "#             generate_date = dates[i:i+20]\n",
    "\n",
    "#             stock_data = df[df.index.isin(generate_date)]\n",
    "            \n",
    "#             lr_fea = stock_data.groupby('Ticker')[['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume']].transform(\n",
    "#                 lambda x: (x - x.min()) / (x.max() - x.min())\n",
    "#             )\n",
    "            \n",
    "#             feature_path = os.path.join(period_dir, f\"{dates[i].strftime('%Y-%m-%d')}-{dates[i + 19].strftime('%Y-%m-%d')}.pkl\")\n",
    "#             lr_fea.to_pickle(feature_path)\n",
    "\n",
    "#             dataset_create = StockDataset(\n",
    "#                 state='create',\n",
    "#                 stock_data=df[df.index.isin(generate_date)],\n",
    "#                 output_dir=period_dir\n",
    "#             )\n",
    "#             image_paths, labels = dataset_create.image_paths, dataset_create.labels\n",
    "\n",
    "#             key = f\"{dates[i].strftime('%Y-%m-%d')}-{dates[i + 19].strftime('%Y-%m-%d')}\"\n",
    "#             value = [[(image_paths[j], labels[j]) for j in range(len(image_paths))],feature_path]\n",
    "#             period_images[key] = value\n",
    "            \n",
    "#             count += 1  \n",
    "            \n",
    "#             k = np.random.randint(5, 61)\n",
    "            \n",
    "#             if i + 20 + k < len(dates):\n",
    "#                 jump_info = {\n",
    "#                     'from_date': dates[i + 20].strftime('%Y-%m-%d'),\n",
    "#                     'to_date': dates[min(i + 20 + k, len(dates)-1)].strftime('%Y-%m-%d'),\n",
    "#                     'jump_days': k\n",
    "#                 }\n",
    "#                 jumps.append(jump_info)\n",
    "            \n",
    "#             i += 20 + k\n",
    "        \n",
    "#         images_dict[window['folder_name']] = period_images\n",
    "#         all_jumps[window['folder_name']] = jumps\n",
    "#         print(f\"Completed window {window['folder_name']} with {count} images\")\n",
    "    \n",
    "#     return images_dict, all_jumps\n",
    "\n",
    "# base_output_dir = r'C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\training\\image'\n",
    "# images_dict = generate_images_by_period(df, base_output_dir, time_windows)\n",
    "\n",
    "# # with open(r'C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\training\\', 'wb') as file:\n",
    "# #     pickle.dump(images_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageDraw\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "\n",
    "df = pd.read_pickle(r'C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\stock.pkl')\n",
    "\n",
    "with open(r'C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\training\\dict.pkl', 'rb') as file:\n",
    "    images_dict = pickle.load(file)\n",
    "   \n",
    "time_windows = pd.read_pickle(r'C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\training\\time_windows.pkl')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder_name</th>\n",
       "      <th>trading_period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-10-16_2020-10-16</td>\n",
       "      <td>2020-10-17_2021-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-15_2021-01-15</td>\n",
       "      <td>2021-01-17_2021-04-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-04-16_2021-04-16</td>\n",
       "      <td>2021-04-17_2021-07-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-16_2021-07-16</td>\n",
       "      <td>2021-07-17_2021-10-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-15_2021-10-15</td>\n",
       "      <td>2021-10-17_2022-01-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-01-14_2022-01-14</td>\n",
       "      <td>2022-01-17_2022-04-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-04-15_2022-04-15</td>\n",
       "      <td>2022-04-17_2022-07-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-07-15_2022-07-15</td>\n",
       "      <td>2022-07-17_2022-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-10-14_2022-10-14</td>\n",
       "      <td>2022-10-17_2023-01-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-01-16_2023-01-16</td>\n",
       "      <td>2023-01-17_2023-04-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-04-14_2023-04-14</td>\n",
       "      <td>2023-04-17_2023-07-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-07-14_2023-07-14</td>\n",
       "      <td>2023-07-17_2023-10-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021-10-16_2023-10-16</td>\n",
       "      <td>2023-10-17_2024-01-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022-01-16_2024-01-16</td>\n",
       "      <td>2024-01-17_2024-04-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2022-04-16_2024-04-16</td>\n",
       "      <td>2024-04-17_2024-07-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022-07-16_2024-07-16</td>\n",
       "      <td>2024-07-17_2024-10-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022-10-16_2024-10-16</td>\n",
       "      <td>2024-10-17_2025-01-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              folder_name         trading_period\n",
       "0   2018-10-16_2020-10-16  2020-10-17_2021-01-15\n",
       "1   2019-01-15_2021-01-15  2021-01-17_2021-04-16\n",
       "2   2019-04-16_2021-04-16  2021-04-17_2021-07-16\n",
       "3   2019-07-16_2021-07-16  2021-07-17_2021-10-15\n",
       "4   2019-10-15_2021-10-15  2021-10-17_2022-01-14\n",
       "5   2020-01-14_2022-01-14  2022-01-17_2022-04-15\n",
       "6   2020-04-15_2022-04-15  2022-04-17_2022-07-15\n",
       "7   2020-07-15_2022-07-15  2022-07-17_2022-10-14\n",
       "8   2020-10-14_2022-10-14  2022-10-17_2023-01-16\n",
       "9   2021-01-16_2023-01-16  2023-01-17_2023-04-14\n",
       "10  2021-04-14_2023-04-14  2023-04-17_2023-07-14\n",
       "11  2021-07-14_2023-07-14  2023-07-17_2023-10-16\n",
       "12  2021-10-16_2023-10-16  2023-10-17_2024-01-16\n",
       "13  2022-01-16_2024-01-16  2024-01-17_2024-04-16\n",
       "14  2022-04-16_2024-04-16  2024-04-17_2024-07-16\n",
       "15  2022-07-16_2024-07-16  2024-07-17_2024-10-16\n",
       "16  2022-10-16_2024-10-16  2024-10-17_2025-01-16"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(time_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from CNN_stock.CNN20 import CNN20d\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from CNN_stock.image_to_tensor import StockDataset\n",
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "def cnn_train(train_tensor_dataset,test_tensor_dataset):\n",
    "    \"\"\"\n",
    "    train\\test_tensor_dataset:圖片集\n",
    "    adj for gcn\n",
    "    \"\"\"\n",
    "    # inital model\n",
    "    model = CNN20d()\n",
    "    device = torch.device('xpu')\n",
    "    model =model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-4)\n",
    "    model, optimizer = ipex.optimize(model, optimizer=optimizer, dtype=torch.float32)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion = criterion.to(device)\n",
    "    print(f\"Model is on: {next(model.parameters()).device}\")\n",
    "    \n",
    "    # early stop; CV\n",
    "    num_epochs = 100  \n",
    "    patience = 10  \n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    np.random.seed(123)\n",
    "    torch.manual_seed(123)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        \n",
    "        train_loader = DataLoader(train_tensor_dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch_images, batch_labels,batch_ticker in progress_bar:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            outputs = model(batch_images)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            predicted_classes = outputs.argmax(dim=1)\n",
    "            train_correct += (predicted_classes == batch_labels).sum().item()\n",
    "            train_total += batch_labels.size(0)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            train_accuracy = train_correct / train_total * 100\n",
    "            progress_bar.set_postfix({'Loss': f\"{loss.item():.4f}\", 'Accuracy': f\"{train_accuracy:.2f}%\"})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        test_loader = DataLoader(test_tensor_dataset, batch_size=32, shuffle=True)\n",
    "        with torch.no_grad():\n",
    "            for batch_images, batch_labels,batch_ticker in test_loader:\n",
    "                batch_images = batch_images.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "                \n",
    "                outputs = model(batch_images)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                \n",
    "                predicted_classes = outputs.argmax(dim=1)\n",
    "                val_correct += (predicted_classes == batch_labels).sum().item()\n",
    "                val_total += batch_labels.size(0)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(test_loader)\n",
    "        val_accuracy = val_correct / val_total * 100\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "            f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
    "            f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "        \n",
    "        # early_stop\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN視覺化檢查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x900 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cols = 3  \n",
    "# rows = 3\n",
    "\n",
    "# fig, axes = plt.subplots(rows, cols, figsize=(12, 3 * rows))  #\n",
    "\n",
    "\n",
    "# for i in range(9):\n",
    "#     ax = axes[i // cols, i % cols]  \n",
    "#     ax.imshow(batch_images[i][0].cpu().numpy(), cmap='gray') \n",
    "#     ax.axis('off')  \n",
    "#     ax.set_title(f'Image {i + 1}')  \n",
    "\n",
    "# for j in range(9, rows * cols):\n",
    "#     axes[j // cols, j % cols].axis('off')\n",
    "\n",
    "# plt.tight_layout() \n",
    "# plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAI3CAYAAABKw+g5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA+1ElEQVR4nO3defyUZdk3/ktZAhdAct930MQN3JfAXUxwN83M3DK3NE0x6866W8ytsCxQUUwNFRPBQBTXBHeRUEHczX1HfRKU7fdX9+v5PfdxXDHTl5Pt/f7zM5zHnDNzzXXN92BecywxZ86cORUAAAAAFLTk/N4AAAAAAIsfTSkAAAAAitOUAgAAAKA4TSkAAAAAitOUAgAAAKA4TSkAAAAAitOUAgAAAKA4TSkAAAAAims9t/9wnXXWmZf7WOS8+uqrYb7EEkukax566KEw32677Rq+/6zWoYce2nCtRcW2226b3rbyyiuH+a233trw/ey1115hPnDgwDBv3Tp/G5522mlhfvHFFze8L8cEc+vll19uem3fvn1bcCeLvttuuy3MW7Vqla7p3r17mD/xxBNhnp3fqqqqVltttTBfaaWV0jW0jFGjRoX5nDlzGq6VfbaYOXNmmK+++uppreyY2GqrrcL8H//4x7/ZHYuq4cOHN722c+fOLbiTRd9FF10U5scee2y6ZtVVVw3ztddeO8wnTpyY1urQoUOYf/bZZ+mahc3GG28c5pMmTWq41iqrrBLmb731VsO1dttttzB//vnnw7zuM9yWW24Z5tnrO3r06LTW8ssvH+YHH3xwmF9zzTVpLRZtH3744b/9N74pBQAAAEBxmlIAAAAAFKcpBQAAAEBxmlIAAAAAFDfXP3Se/XD3fvvtl67p169fmC/OP6w8e/bs9Laf/OQnYb7sssuG+aeffprWuuGGGxrbWBOa+XHTlvwx91mzZoV59gPBW2yxRVrrsMMOC/P+/fs3vK/sxwKzHzTPHkdVVdVTTz0V5s38EO4+++wT5scdd1yYDxkypOH7aMljYnE+TyzMRo4cGebt2rVL12yyySZhvjj82Hb2Xq57jz/22GMN3UfdgI0XXnghzFvyuc9+0LtOoz/mXvd8jR8/Psyza0L79u3TWtmx+uijj4b5kkvm//f3pz/9KcyPOOKIMK/7vDV9+vQwz36ktmPHjmmtYcOGhfmAAQPCvJnzfkseE4vDeWJRtMEGG4R53fkt++HuRenHthtVd+7LflT7+uuvD/OePXumtbLzZfY5tRnZD4038wPs2Y+5z5gxI6215pprhvlSSy0V5nfddVdaK1vTzA/WZz9c3qNHjzCfMGFCWuu+++4L87/97W9hPmLEiLTWF198EeaXXnppmNdddzIteUwszueJhYFvSgEAAABQnKYUAAAAAMVpSgEAAABQnKYUAAAAAMVpSgEAAABQnKYUAAAAAMXFM+obkI0Orqp8HOfirJnR3DNnzgzzujGw9957b5hno5sPOeSQtFY2ivTVV19N12TOOOOMhv795ptvnt72/e9/P8yz56XuWN1uu+3CvFu3bmH+9NNPp7X+8pe/hPmsWbPSNZns9VprrbVarNb5558f5ttss01aa5999gnz//qv/2p4XwcddFCYv/766y22r0GDBjW8rwXVMcccE+YLymOcPXt2mH/++efpmuzctziPeq+7VmSjprNxx1/5ylfSWi+99FKYjxo1qmZ3jcmuYa1atUrXZGPhs+Nr6aWXTmvtsssuDdWaPn16Wuvxxx8P8+yxPPvss2mtU045paFa06ZNS2ttvfXWYd6mTZt0TSYbi55dW+uO1S5duoR5dj2sOyaeeOKJeb6v9dZbL11Dy/rOd74T5tl7v6ry1+epp55qkT0tarL3xuTJk8O87j3zwAMPhPmAAQPC/Mwzz0xr/fOf/wzznXfeOczHjRuX1so+Q+60005hnn2Gqqr8/H7PPfeE+ZgxY9Jab775Zph/4xvfCPNjjz02rZWd+4477rgwb+bzYJ8+fcK8U6dO6ZoPPvggzHv06BHmU6ZMSWutttpqYX700UeHed3rePjhh4f5k08+2WL7euutt9I189PGG28c5pMmTSq8k8b5phQAAAAAxWlKAQAAAFCcphQAAAAAxWlKAQAAAFCcphQAAAAAxf3H0/cWd9mv/2dTyOom5u27775hXjc1LrPBBhuE+cSJE8P8xhtvTGtdeeWVYZ5NSKl7jA8++GCYZ1OQskkJVVVVX/3qV8M8mx5SN1WkX79+Yd61a9cwz6YTVVVVfelLX2p4TSabptWMAw88MMyHDh0a5g8//HBaa4sttgjz7DnOXt+qyo+9HXbYocX2lVl55ZXT21pykl82ee7tt98O80MPPTStlU07aUbr1i1/Cah7/9My2rZtG+a9evVquFY24fS5554L8+WWWy6t9emnn4Z5M8dZNgkoux527tw5rZWdR5dcMv5/uey+qyqfRputya6fVVVVW221VZg3MzFvwoQJDa/JHHDAAWGeXSuy57Gq8qlG2Xmi7vzR6PWlmX3N7+l72dTLxWnCYN0xkE2Aq5sQNj8tqJOwBg4cGOZ1z+Oee+4Z5tkEuv/zf/5PWutnP/tZmI8ePTrM647zO+64I8xXXHHFMG/mM0o2bbxuX9lt2WT6Zo7h7G+XUu+HbHJ7+/btw7xusuZmm23WInuqqqq64oorwrx///5hfuqpp6a1sn3N7+l72VTl7bffPszrJlhmUy+bmWr65S9/ueE1/+KbUgAAAAAUpykFAAAAQHGaUgAAAAAUpykFAAAAQHGaUgAAAAAUpykFAAAAQHEtPw98LmSjNb/xjW+ka0455ZQwrxud3qhsX3WysZvf/OY3w/y6665r+D6akY3dbGYU6tNPPx3mP/3pT8N80KBBaa2llloqzLPnPhvlXVVVNWLEiDBfc8010zWN7qvu/jPNrCkhOybqRtpmXnzxxTBvZjT3kUce2WL7WmeddcI8O7622GKLtNYee+wR5v/1X/8V5nX7ffTRR8N86623DvMbbrghrZUd36+//nq6ptF9LShGjhwZ5nXPdffu3cN8pZVWapE9VVVV3XbbbWFed5xne27dOr4MP/vss2mtww8/vGZ3jZk+fXpD/36ZZZZJb8se/xdffBHmvXr1Smt169YtzLPnPsurqqqOP/74MF955ZXDfNasWWmtPn36hPmECRPSNQub7JjIPj/UvR+zNdmxctBBB6W1stfllltuCfPss0tVVdURRxwR5tk5p127dmmtTTbZJMwff/zxdE3mwQcfDPNtttkmzFu1apXWmjJlSpg/88wzYZ6di6qqfpx4Sdmxtu6664b5+PHj01obbbRRmLfkqPddd901zLPXuaqq6qijjgrz7LE381mpmb93smtS9vfZzTffnNbKPkdtttlmYZ79rVVVVTVt2rQw33jjjcO8mefrN7/5TcP7ynz++ecNr1lQ3XHHHWF+0UUXhfnVV1+d1nrggQfCfPTo0WF+zDHHpLVWW221MO/QoUO6ptF9XXzxxWF+xhlnpLV22mmnMM8+Iz3yyCNprc033zzMm/msO3bs2DDfd999w7zu/PHaa681fP//4ptSAAAAABSnKQUAAABAcZpSAAAAABSnKQUAAABAcZpSAAAAABQ3T6fvbbXVVg39+7rJdNnksOzX9+smwO23335h3r9//zCvm1j30ksvhfnf/va3MK+bDDds2LD0tnmtbhpFNtHsRz/6UZh37tw5rZVNplthhRVqdherm5DUqAV1Yl4JzUwevOuuu8K8bvpYpm3btmHezBTFP/zhDw39+7r33OTJkxu+/8zee+8d5u3btw/zuuc+m7LXzGTNbHrpCy+80HCt/8SXvvSlMM/e43UTp7IJUvfee2+Y102AyzQzhaxjx45hvsMOO4R5dj2qqnxqXEvKHkuPHj3SNdnUtvXWWy/M//GPf6S1sml2vXv3DvNsmlNVtezztShN2WtU9t7afvvt0zXZJKDs3Pvxxx+ntbJppdnkorPPPjutlU1nyqa11U3ManTKXt2EwQsvvDDMs+e+7vz13HPPhXl2DZkxY0ZaK5v+Vzcdal7IXrfsc2rd9L1Gr/F1k9ZWWWWVMF9rrbXCvO56PXjw4DC/9dZbG95XS2pmYl+jevbsGebXXHNNw7VOO+20MG/m+Srx2Bcl2eTPus9IXbp0CfPs+lJXKzvHZZNHmzkm3njjjTCve29nU+4efvjhMM+ueVVVVcOHDw/zH/7wh2HezJTcs846K8x33HHHtNZ/wjelAAAAAChOUwoAAACA4jSlAAAAAChOUwoAAACA4jSlAAAAAChunk7fu+CCC8K8mclF2W0//vGPw3z//fdPax122GFhnk07qtvXwiabtDZt2rR0zYABA8J89dVXD/PFeZLdwqiZyYfNTOzLlDhemtlvNrmomfNB9hy35GNv5jG25ATLf2nm+Xn33Xcb+vfZpKWqyqeYZNedJ598Mq216qqrNrSvljS/p/1kEw7rrhV1k2IiddP3WPBkkw/rbLTRRmFeN80uU2LyYXbc10256969e5hfdNFFYb7++us3fP/ZhME62WfdbOJt375901olJn7OjWzi1ZAhQ8K8TZs2aa1sWvC3vvWtMM+m4lVVfv1dHDQzaS3TzJQ9Fjx33nlnmNdNuXvnnXcauo9mJuZdccUVDa/JZOfquink2efQESNGhHkzEwYHDhwY5s08X1/72tdarNbc8E0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACguNZz+w+bGe2ZjY7OxiXOnj07rbX33nuHeTaKdNiwYWmtgw8+OL1tUZeNpq/TkmPrWTQsbMdEM8d9SyrxfC0or0k2Jne33XZL15x33nlhvu2224b5K6+8ktbKRtV+8MEHYf7WW2+ltWbNmhXmzVwPFza9e/dueM2ECRNafiMs1Ba2Y6KZ437SpEkN1Xrqqacavo9mZPtaULRuHf8J0sxI9UsvvbTh+89q7bjjjmF+9dVXp7WyPU+fPr3hfS1srrjiijCfV2PjWfC9+uqr83sL81x23Hfs2LHhWpdddlmYN/MeWpife9+UAgAAAKA4TSkAAAAAitOUAgAAAKA4TSkAAAAAitOUAgAAAKC4uZ6+t+aaazZcPJuY99xzz4V5MxONmllzwQUXhPnRRx8d5ldddVVaa8SIEWGe/fp93X4feuihMD/00EPTNQALkmamV/3yl78M8+x8ufvuu6e1ll566TC/5ZZbwryZqYDZdKbHH388rXX88ceH+ciRI8O87lrRvXv3MF9ppZXSNQALktNOOy3M66ZXrbLKKmHezJSq1VdfPcyzqYB15+TXX389zE888cQwP/nkk//N7v63Ll26hPkjjzySrtloo43CvG7qLMD84JtSAAAAABSnKQUAAABAcZpSAAAAABSnKQUAAABAcZpSAAAAABSnKQUAAABAcfHc08Crr74a5nUjUg877LAw79+//9ze7f846qijwvy3v/1tmNfta+DAgWG+7bbbNlzrz3/+c0O1Zs+enda64YYb0tsadcwxx4T5T37yk4Zr/fSnPw3zQYMGNVwr25da816JY6KZWg899FCYH3rooQ3XYv677bbbwrxVq1bpmu7du4f5nDlzwnzMmDFprew4z9x1113pbR999FFD++rRo0daK3uMs2bNCvNsLHlVVdULL7wQ5iuttFK6plGjRo1qeE02sny99db7T7fDAqAlj4kpU6Y0XCt7D7XkcU85F110UZgfe+yx6Zqlllqqxe7/9ddfD/PRo0eHed3fAqusskqYb7fddg3va6ONNgrz4447LswffvjhtNbkyZPDfPvttw/zSZMm/Zvd/W8bb7xxmE+cODFds8Yaa4T5csstN8/3tSjVakklHmNLHhN1tTp06BDmn332WbqG+c83pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAobok52RghAAAAAJhHfFMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAorvXc/sMlllhiXu5jkXPUUUeF+eDBg9M1v/jFL8L8iiuuCPOVV145rfXJJ5+E+aRJk9I1C5u11147zFu1ahXmL774Ylpr0003DfPseXzllVfSWqeeemqYd+/ePczPOOOMtNZhhx0W5ssvv3yY33777WmtzNJLLx3md999d8O1WDTMmTOn6bXZ+4/YddddF+aXXnppuuaRRx4J85VWWinMN9xww7TWM888E+YfffRRumZh06FDhzDv0aNHmD/00ENprWnTpoV59hmp7r207rrrhvlPfvKTMB81alRaa/XVVw/zCy64IMw333zztNbEiRPD3OdA/l+zZs1qem3r1nP9JwhVVQ0YMCDMjzvuuHTNoYceGubjxo0L8zfeeCOt1bt37zAfPXp0umZRscYaa4T5a6+91nCtXXbZJczvueeedM2BBx4Y5uPHjw/zF154Ia2VHRM9e/YM8xNPPDGttTgfEzRm5syZ//bf+KYUAAAAAMVpSgEAAABQnKYUAAAAAMVpSgEAAABQnKYUAAAAAMXN9eiLa665Jsy/9a1vpWs23njjMF+UJsBlPvvsszDPprxVVVXtvvvuYb7zzjuH+U477ZTW+tWvfhXm55xzTrqmUc1MGMyOia233jrMx44dm9bq1KlTmGeTJW677ba01t577x3mU6ZMCfMJEyaktbKJeb/+9a/D/P33309rZc9LNvGjbqLSmWeeGebrr79+uqZRLXlMLA7niUVRdq0YPnx4uubee+8N80VpAlyj2rdvn96WTUF7/vnnw3yrrbZKa3344YcN3Uczss8JdRNRs6mAffr0CfPsXFlVVbXaaquFebt27cL8oosuSmuNGTMmzLMpim+99VZa67LLLgvzbt26hfnPf/7ztNaQIUPC/N133w3zzp07p7Uy2TGZTSSs05LHxOJ8nliYrbPOOmH+5ptvpmt69eoV5ovztK+6c/Vdd90V5jvssEOY103fa9OmTWMba8IBBxwQ5jfffHO6JpsAt9RSSzVcK5tq/vLLL4f5kkvm3+3I9rXffvuFeXZtqaqqWmWVVcL8ySefDPNll102rZUdE23btg3zuuMrmwjbklP5WvKYWJzPEwsD35QCAAAAoDhNKQAAAACK05QCAAAAoDhNKQAAAACK05QCAAAAoDhNKQAAAACKaz23/3DkyJFhvtlmm6Vrvv71r4f5OeecM7d3u8ipG1382GOPhfnf//73MD/33HPTWj169Ajz8847r6G8qqpqk002CfPHH388zLOR3VVVVSeeeGKYDxs2rKH7rqqq2mabbcI8G+k6derUtFY28vSwww4L82effTat9cILL4T55MmTw7xLly5prWzk6YQJE8L81VdfTWs9/PDDYb7++uuHefY4qqqqNt100zD/7LPP0jWZCy+8MMy/973vtdi+Jk6c2PC+Slh77bXD/JVXXim6j5bUqlWrMH/++efTNR9++GGY140iXtR17do1ve3ee+8N87POOivMTzrppLTWqaeeGuZ9+/YN8w8++CCt9fbbb4f5j370ozD/zne+k9bKjon+/fuHed3niu9+97th/uc//znMZ8yYkdbKfPLJJ2F+++23p2uyMePbbrttmLdr1y6tlb1Xrr322jDv0KFDWiuzxx57hHn2nq+q/Fjdfffdw3z8+PFpreyYyEasN7Ovus9o81P2emXXvKqqqmeeeSbMF5THePbZZ4f5cccdl65p06bNvNrOQmvOnDnpbcsuu2yYH3300WF+2223pbWGDx8e5ieccEKYDxgwIK11yCGHhPlSSy0V5nWP8amnngrzHXbYIV2TOfDAA8P8mmuuabjWW2+9FeatW8d/etd93vnd734X5i+99FKYd+/ePa2VnWOz62Hdvg466KAwz657X/rSl9JaG2ywQZhn152hQ4emtZZZZpkwz46vWbNmNbyvp59+Ol2zsMn+1i79GH1TCgAAAIDiNKUAAAAAKE5TCgAAAIDiNKUAAAAAKE5TCgAAAIDi5nr6XiabDFdV+fS9BVWJSVivvfZaels2veemm24K844dO6a1fvGLX4T5kUceGea9evVKa33/+98P81NOOSXMswlMVVVVY8eODfPddtstzLOJE3WyiS51UwH32muvMB80aFCYP/LII2mtbMpdM6ZMmdJitbLj+OCDDw7zbIphVeXPZTaRcfPNN09r7bzzzmF+8sknh3k2kbCq8kl+2ev78ccfp7VaUjbhMJtA9cc//jGt1a9fvzC/4YYbwrzu/FX3upSUTdZZUKfvteQkrMyDDz7Y0L+vqqq6+uqrw3z55ZdP12y99dZhnk2Ay46/OgMHDgzzu+++u+Fa2fS9uvdM5osvvmh4TSab8Fd3DGe3Zfuq22+j75Xs80ZV5cdE9hj333//tFY2kXXFFVcM8+wzSlVV1Zlnnhnm2TV/1113bXhfxx9/fJjXTZ1syUl+2cTf//7v/w7z3r17p7WyaYVLLhn/f3Tbtm3TWnXTz0qqmw63qKibOt2o7G+OK6+8MsxXWGGFtNZ7770X5tmk1rpzcvb+O/3008N88ODBaa3sMR5zzDFhPmTIkLRWtucbb7wxzOvOu9mkt2wabfa+rDNq1KiG1zRzP5nsdcymO2bX76qqqnXWWSfMs/dD3XM/bty4MM+Oo2zqYt2+ssl0zUwYbGbKXdYD6NSpU5hn036rKn+M2XX6nnvuqd9ck3xTCgAAAIDiNKUAAAAAKE5TCgAAAIDiNKUAAAAAKE5TCgAAAIDiNKUAAAAAKK71vCy+4447hnk2pnPQoEFprWzU9sSJExvfWKJnz55h/re//S1dM3ny5DA/9dRTw3y//fZrdFvV1KlTG8qrKh8JmY2qrBt3nD3G7P7rRtA/+uijYb7qqquGeTamsqqq6o477gjzbETpgAED0lp1ty0qstfr448/DvOxY8emtX73u9+FefY+rTvus1G7zzzzTJg//PDDaa0RI0aEefbYN99887TW3nvvHeZvv/12mN95551prT333DPM33zzzTDfeeed01oHHHBAmLdv3z7Mp0+fntbKxr0uKK655powHzp0aLomGxleNz64URdeeGGYv/TSS+ma7Lz0ta99LcyvuuqqtFb2WL744oswz46zqqqq999/P8yzEfTZdbKqquq+++4L8+zano00rqqq+uSTT8J8jTXWCPMePXqktR577LEwP+GEE8K8Xbt2aa2WPI4WVNnzlR0rRx55ZForu7ZmnxMeeOCBtNacOXPC/Nlnnw3z4447Lq2VjSbPjrssr6qq+vDDD8P8qKOOCvOtt946rXXTTTeFeXY9XGWVVdJazz33XJjPnDkzzPv165fWys7FC4pddtklzMeMGZOu2WeffcJ89OjRLbKnqsqv1zfffHO6Zptttgnz7Bho5px0++23N7wme75ee+21hmuNGjUqzLP3+JJLNv4diiuvvDLM656v7P4HDx7ccK2HHnoo39wiIjuOsvN+3fP18ssvh3n2t16dN954I8yzz451+3r88cfD/LTTTgvzSy65JK2V/U274YYbhnndZ93sM//9998f5tlnp6rKewPZ3y5159W11147ve3f8U0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIqbp9P3sql1M2bMCPOLL744rbXddtuF+fbbb9/wvrIJYdl0pGy/VVVVjzzySJgPHDgwzLNfy6+q+ml6jWp00lrdFMNsukv37t3D/O67767fXCCbDlU3NYqWkb1Ps4mIVVVV119/fZhn007qJnZdd911YZ5N8urSpUtaa9KkSWF+/PHHh3k2aaWqqqpv375hnr2HsuksVVVVW2yxRZhn55Zhw4altYYPHx7myy67bJjXTSucNm1amGdTReaVbPJJ9v6fNWtWw/ex3HLLhflHH33UcK2ll146zLOpJ1WVT3Dp1q1bmNdNNGrJCXDZxL5s0lrdOTl7P2XTyZp5HEcffXTDaxqdVpjli7tsSm/2eafOySefHObNTD7MJsPVvR+zWtlnyltuuSWtlcmmTp533nnpmnvvvTfMs4nK7733Xlpr9uzZYf7d7343zC+//PK0Vt21sqSDDjoozL/97W+Hed2UqOeffz7Ms4l5zRwDe+yxR5jXTdW69dZbwzx7/zUzma4Z2aS1bMpk3fk9OzbrXq9GZc9xM89XM9MKF2fZ56062d+hdVPjGpV9ft9yyy3TNRMmTAjzrC9RJ/v8nl3D6qZ0tmnTJsyfeOKJMK97P2ZTX7O+SF2tf/zjH+lt/45vSgEAAABQnKYUAAAAAMVpSgEAAABQnKYUAAAAAMVpSgEAAABQ3DydvpdNudt8883DfJ999klrPfnkk2GeTfXp3LlzWuuyyy4L8z/84Q9hXjcFady4celtkWzaVVXl08Za0p133hnmdRPNsl//b2bKHgue6dOnh3ndNMiLLrqoofsYPXp0Q/++zpQpUxq+LZtc1LFjx7RW7969w/zII48M8/PPPz+tlU3me/rpp8N8rbXWSmv98pe/DPMFZTpSM7K9Z1MLb7zxxobvY8iQIWFedyxnk4B22WWXML/qqqsa3lfd9WV+uuKKK8L8nXfeSddkE1laclog888PfvCDMO/QoUO65pNPPgnzZiYfZsdR3ZS9RmUT1uqm3GV23XXXMM8+z1ZV/hiz6V/LL798WiubMlY3Za/Rff0nsglddff1z3/+M8ybmY624YYbhnn2eSGbGFdV+eeFZmRT9hZU2eebuolm48ePD/OZM2e2xJaYz7JjoiUnHzZTK/sbuBnZY1xhhRXSNdl1ZPLkyQ3ffzblLpuCXrev7JwzatSoMJ9XEz99UwoAAACA4jSlAAAAAChOUwoAAACA4jSlAAAAAChOUwoAAACA4jSlAAAAACiu9dz+w7Fjx4b5ueeem6559NFHw/ztt9+e27v9H9lI+brRupnWreOHvdNOO4X5G2+80fB9LKimTp3aUF5VVTVlypR5sxkWCHWv/aKimeM+079///9sM3NhYX5NspHuxx57bLrm1FNPDfPPP/88zGfNmpXWmjNnTpi///77Yf7OO++ktTLZONzp06c3XGtBdf/99ze8Zl6MjWfBkb23s3xh1JLHfUvWyp7jN998s+H7WFDcfPPNYV43unzkyJFhno1Hr7teZ6PmBwwYEOZXXHFFWuuhhx4K81NOOSVds6jInsc682qkPAuGZo6JhU1LHvcXXHBBQ/++zsL83DsrAAAAAFCcphQAAAAAxWlKAQAAAFCcphQAAAAAxWlKAQAAAFDcXE/fa9u2bZj/4he/SNdsuummYd61a9e5vdv/kU3jaGYq4KhRo8J85syZYV436emOO+4I86OOOirMBw8enNbKnq+JEyemawAWJMccc0yYZ1PxqiqfOPXpp5+2yJ6qqmWnAp500kkN3UdV5Y/xW9/6VphPnjw5rfXII480dB8AC5oPPvigobyq8mlU2TmxGcstt1yY151fP/roozDPplfX1cqulQceeGCYDx06NK21zz77hHk20RxgfvFNKQAAAACK05QCAAAAoDhNKQAAAACK05QCAAAAoDhNKQAAAACK05QCAAAAoLjWc/sPe/bsGeaDBw9O1xx22GFh3rZt2zC/6aab0lrvvfdemO+xxx5hPmTIkLRWu3btwnzrrbcO83HjxqW1Nt100zBfccUV0zWZ7Pn65JNPwvyVV15p+D7WXnvthmtlj3F+72thq9WS5vdjzI6JLbfcMszrzhMbb7xxmE+aNCldw4Lr2muvDfNLL700XdPoOO811lgjvW3ixIlhfv/994f5zJkzG7rvqqqqZZZZJsyzceV1dt999zAfP358uiYbGd6xY8cwz96vVVVVzzzzTJjPmjVLrflUKxsv34wOHTqE+fx+jH369AnzF198seFaLfl8Uc4hhxwS5mPGjEnX7LPPPmGefR5txo033tjwmt69e4f52LFjwzw7h9fVyv7eGTp0aFpr9OjR6W2N2mSTTcI8+3vnrrvuSmtl54ynn366xfal1rxX4pjYcMMNw/zmm29Oa2XvoZZ8P9DyfFMKAAAAgOI0pQAAAAAoTlMKAAAAgOI0pQAAAAAoTlMKAAAAgOKWmFM3AgIAAAAA5gHflAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIrTlAIAAACgOE0pAAAAAIprPbf/sF27dvNyH4ucX//612F++umnp2uOPvroMH/22WfD/MEHH0xrLbHEEmHepk2bdM3CpkePHmH+3HPPhfknn3zS8H3stddeYT569Oh0zfLLLx/m3bp1C/M777wzrTVnzpwwz17f2bNnp7Wyx3Lvvfema1g8TZ8+vem1nTt3bsGdLPqmTp0a5r/61a/SNWPHjm0o33///Rve16233trwmkXFhx9+mN62ww47hPnkyZPD/KqrrkprXXzxxWE+ceLEML/ooovSWksuGf8f45lnnhnmdcfEdtttF+Znn312uobFU9175d/JPscQO/jgg8M8O/dUVVV985vfDPOZM2eG+bnnnpvW6tq1a5hnf6MsDpZaaqn0ts8++yzMs7+n6z537bbbbmGevYfGjBmT1tp7773DfMcddwzz9u3bp7VmzJgR5q4V/L+yv2f/b74pBQAAAEBxmlIAAAAAFKcpBQAAAEBxmlIAAAAAFDfXP3Se/ZhZ3Q9XLQ4/tt2ouudr3333DfMnnnii4fvJatX9QHejsh8ab+YH2LMfVn3ooYfSWtlt2WPcc88901p9+vQJ8169eoX5iBEj0lrZD85mP2R/xx13pLWGDRsW5jfccEOY33jjjWmtDTfcMMw7derU0H3XacljYnE+TyzMsnNP3Q8rZ++nxeHHtrNrwnnnnZeuufTSS8N85MiRYf7ee++ltbbZZpt8cy0ke4z9+vVL10yZMiXMs/PSD3/4w7RWly5dwjw7Jq+88sq0VvZD/tkP1mePo6ryH7zNhnJ85zvfSWtlP4T78ccfh/ngwYPTWtdcc02Yr7POOmHezI9dt+QxsTicJxZFu+++e5jff//96Zp11103zBeHH9vOBtlsscUW6ZrsXLLpppuG+THHHJPWyn64uyWf++zH3IcOHZquyX6APRsuNH78+LRW9hn2l7/8ZZjvs88+aa3sdcl+ZH799ddPa02YMCHMd9111zCv+7w1bdq0ML/nnnvC/Le//W1a6/jjjw/zFVdcMczffffdtFamJY+JxeE8sTDzTSkAAAAAitOUAgAAAKA4TSkAAAAAitOUAgAAAKA4TSkAAAAAitOUAgAAAKC41nP7Dy+55JIwP+2009I1ffr0CfPRo0fP7d0ucpZYYon0tnPPPTfMTz311DCvGw99wAEHhPnyyy8f5ldddVVaK9tz9vqOGzcurXX44YeHeTY+tK5W3759w/zNN98M82wEdVXlo6Zvv/32dE1mwIABYb7UUkuFeevW+dvwhhtuCPMTTjghzLNx6VVVVWuuuWaYP/fcc2H+xRdfpLUy2ejWutfx6KOPDvM33ngjzO+4446G99W2bduG15SQjR9+/PHHC++k5fzpT38K886dO6drsjHfi7PPP/88ve28884L8+wc99e//jWtNWvWrDDv0KFDmL/yyitprW9/+9thnh0TddfD733ve2H+2GOPhXndmO977703zLMR66ecckpa6/e//32Yjxo1KszPOeectNZaa60V5tnzUvd83X333WHeqVOndE1m6tSpYb7DDjuEed348REjRoT54MGDw/zWW29Na2Xj1/fdd98W21fd/S+IssdeVVXVqlWrMF9QHmP2+earX/1quqZbt25hvjiPeu/YsWN62zbbbBPm2WfFLl26pLVeeOGFMM+uFTNnzkxrrbHGGmG+6aabhnnd9fCkk04K82eeeSbMd95557TWO++8E+ZnnXVWmG+11VZprezc//zzz4f55MmT01q77bZbmJ9xxhnpmkz299a7774b5iNHjkxrZdeXrl27NnQfVVVVbdq0CfPs9cr+dqmq/G/N7Lh/+OGHG97XjBkz0jULmwXlMfqmFAAAAADFaUoBAAAAUJymFAAAAADFaUoBAAAAUJymFAAAAADFzfX0vcVBiUlYdRPg1l9//TDPJjJk0w2qqqquv/76MP/KV77S8L6yCXTZFINsAltVVdWRRx4Z5nUTnRp11113hXnd5KIxY8aE+Q9+8IMwr5sgee2114Z59nwtuWTeG87W1E3Zy7z++uthXve8ZL785S83vCaTHffZ+7Fu+t6ee+4Z5tn0rVKyx9LM5MPs9cqmMC0o004HDhyY3tazZ89yG5mHmpmEdfXVV4d5u3bt0lqbbbZZmE+bNq1md7Hs/ZRdE3r16pXWWm655cI8m3LXr1+/tFZ2W7avbLJsVVVV+/btwzx7L9VNejr++OMb2lfd9LuPP/644TUlZM9LM5M1s+lf2fS9p556Kq21xRZbtNi+tt566zDP3sN118mWnOSX3U92TbjmmmvSWj//+c8buu9Bgwalt2UTiv8T2XtpcZdNwsrcd9996W0ffPBBmA8ZMqSh+6iqqtp9993D/P333w/zbJJeVVXVdtttF+ZbbrllmNdNGMw+j6666qphnn3er6qq+uyzz8I8ux5nj6Oqqurss88O80ceeSTMs+tkVdX/ndCougl4jcpel2xiXzZNvqryc382ibt3795prWyCZ/b36Y9//OO0Vjbxt276X6Ylp9xtsskmYX7ooYeG+YsvvpjWeuCBB8I8e+6nT5/+b3bXHN+UAgAAAKA4TSkAAAAAitOUAgAAAKA4TSkAAAAAitOUAgAAAKA4TSkAAAAAims9L4tnY9Az2VjdqsrH5DY6OrVONjq1mfHsr776akP/vqry8bgHHHBAuiaz1157hflaa63VcK0//vGPYb7OOuuEed3rmI1Fz0ZYNvN8ZeNe27Ztm9b6wQ9+EObDhg1L12SaWVNC9jr++te/DvPbb789rXXCCSeE+S233BLmda/jSiutFOa77rpruiaTPZZddtklzD/88MO0VrbnbGT4bbfdltZ68MEHwzwb0bruuuumtY4++ugwv/rqq8M8G1VfVVW18cYbp7ctCGbNmhXm2YjeqqqqM844I8zHjRvXInuqqqqaOnVqmNeN3F1//fXDPDvOPv/884b31YxspPNHH30U5g8//HBa67e//W2Yd+rUKcxvvfXWuq2Fsuf+/vvvT9dk171sX4u7Ro+J/v37p7UGDhwY5tl5qe64Hz58eJhn76G6fV1xxRVhfuCBB4b55ZdfntbKztfZsfqXv/wlrXXDDTeE+U033RTmvXr1SmsdfvjhYZ4999kY96rKR4b369cvXVNSdi2bNGlSuqZr165h/uyzz7bInqqqqn73u9+F+TvvvJOuWWaZZcI8+xul7rNtS8o+Wx922GFhvsMOO6S1PvvsszDfb7/9wnzGjBlprRVXXDHMV1hhhTDPnt+62+6+++4wv/766xve17vvvpuuWdhkf+tmf5+tssoqaa2///3vYZ79PZt9rq+qqvrNb34T5ltuuWWYL7300mmt7Ly47LLLNryv2bNnh3m3bt3CfPz48Wmt999/P8z79OkT5tnf2VWVv14zZ84M8+zvtqrKj/u54ZtSAAAAABSnKQUAAABAcZpSAAAAABSnKQUAAABAcZpSAAAAABS3xJy60Uz/l2yKyWmnnZauueSSS8I8m6Jw9tlnp7VWXXXVMP/Rj34U5qeffnpaK5NN7qqr9b3vfS/Ms2kBffv2TWsdddRRYT548OB0TaOaeYzZdJWrrroqzLOJMyyYsmk/dVMff//734f5nnvuGeYbbLBBWiub+pC9h77+9a+ntbLJHtlkrrpplNn933PPPWFeNx1m0KBBYb7ccsuFeTYBqqryiXT33XdfmNdNAvnrX/8a5r17907X/DvZJKz27duna7IpaG+//XaY9+zZM62VTRh55JFHwnzDDTdMa2UTFbNpinWPMdtz3TS7+SmbtFY3STObtJZN5cxeExZMzVzbswlFn376aZjXHV/ZZKxsqm8zsuO+Xbt26Zps0tOJJ54Y5l26dElrPffcc2GefVSv+wi/5JLx/zvP5cf+/5/sWvnyyy83XOtfDjrooDCfMmVKuubkk08O8+WXXz7M684xrVvHA8iz4zybYFwnu5bXPcZsEtedd94Z5t/4xjfSWtddd12Y100eblR2/3XT96ZNmxbm2fRcFi7ZMfHCCy+ka7bffvswzz7z1k1Ufu2118K8mcmH2efTc845J8zrzq8bbbRRmF955ZVhvttuu6W11llnnTDPJkJuscUWaa3s+comJNdNR3/jjTfC/K677krX/ItvSgEAAABQnKYUAAAAAMVpSgEAAABQnKYUAAAAAMVpSgEAAABQ3Dydvjd69OgwzyZhnX/++Wmt22+/vaF9ZfddVVX1s5/9LMyzKTF1jzGbpld3//NTNmntkEMOSdfUTQKLDBs2rKF/z/z13e9+N8zrJs3sv//+Yb6gvvbZcZ9Nz6iqqjr88MPDfOeddw7zXr16pbWyKXdjxowJ87rzRzaRLpsW2MxrMn369IbX/Esz0/eyyXTZeb+u1ueffx7m2aXu8ssvT2sNHz48zEeOHBnmdZMhs4lOC+r0vWy/ddPRMh07dvwPdwNllJgeXPceWnPNNcP8448/nlfb+Y/8J1PcjjjiiDDPps9VVT6964MPPgjz7LNKVVXVN7/5zTDPJg937tw5rZU9D9kExNmzZ6e1skmPQ4YMSdcsbFZcccWG/n0zU9NYuGTHxML22p911lnpbT169AjzSZMmhfm1116b1lp77bXD/O677843l8iml2bnr2Zek7lpN/mmFAAAAADFaUoBAAAAUJymFAAAAADFaUoBAAAAUJymFAAAAADFaUoBAAAAUNwSc+ZmRl9VVaeffnqY140u33333cP8zDPPDPO11lorrdW3b98wz8awn3baaWmtSy65JMxfeumlMP/973/f8L7qnheABdn06dObXpuNkK2TjUjPxma3b98+rdWzZ88wv/322xuuteuuu4b5X//614buu+5+Hn744XQNwILsww8/bHrtJptsEua9e/dO19xzzz1hfsghh4R53Uj1bt26hfkXX3wR5lOmTElr/fCHPwzztm3bhvmIESPSWjNmzAjzIUOGpGsAFmRz027yTSkAAAAAitOUAgAAAKA4TSkAAAAAitOUAgAAAKA4TSkAAAAAims9t//w5Zdfbiivqqq6/PLLw3y99dab27v9H9mUu2wSR51sClL//v3D/LLLLktrHX/88Q3dd93EjWwCVZs2bRq6D4D5pVOnTmF+1VVXpWsuvPDCMH/wwQdbYktVVeXn12nTpqVrZs2a1VCt+++/P62VTZedOnVqmN9yyy1prUGDBoX5uHHj0jUAC5IXX3wxzLPrQZ2uXbuGeTbhr067du3C/IgjjkjXZH+L3HHHHWG+6qqrprWWWWaZMD/44IPDfOjQoWmt7Hl59tln0zUA84NvSgEAAABQnKYUAAAAAMVpSgEAAABQnKYUAAAAAMVpSgEAAABQnKYUAAAAAMW1ntt/2KtXrzAfMWJEuqZPnz6N7ygxfPjwMD/ooIPC/M9//nNa66STTgrz9u3bN7yvfffdN8wvueSSMM8eR1Xlz9f7778f5o8//vi/2d3/ttdee4V53euY2X777cO8mX316NFjka/VklpyXyWOiQcffDBds8QSS4R5mzZtGr5/5r+pU6eG+ZQpU9I1nTt3bug++vbtm9728ccfN1Tr/PPPT28bO3ZsQ7VuueWW9LZBgwaF+SuvvBLmhx9+eFqrZ8+ejWwrvU5VVVW1atUqzIcNGxbmV199dVorO2fceuutLbavxblWS5rfx0RWq1+/fmmt7BxS4vmi5WXH4OTJk9M13bp1C/Pp06eHed11Z8sttwzzadOmhXndsZl99tl2223DfOjQoWmtrl27hnmnTp3CfJNNNklrZc/Xs88+m67JZJ/JsvNF9prU1ZoxY0aL7Uutea/EMbHffvuFeTPvoWaOe8rxTSkAAAAAitOUAgAAAKA4TSkAAAAAitOUAgAAAKA4TSkAAAAAiltizpw5c+b3JgAAAABYvPimFAAAAADFaUoBAAAAUJymFAAAAADFaUoBAAAAUJymFAAAAADFaUoBAAAAUJymFAAAAADFaUoBAAAAUJymFAAAAADF/X9xeW87mcFb0wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x900 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# layer_outputs = model(batch_images, return_intermediates=True )\n",
    "# layer1s = layer_outputs[2]\n",
    "\n",
    "# layer_outputs = model(batch_images, return_intermediates=True )\n",
    "# layer1s = layer_outputs[2]\n",
    "\n",
    "# i = 7\n",
    "# feature_maps = layer1s[i]\n",
    "# feature_maps = feature_maps.cpu().detach().numpy()\n",
    "\n",
    "# size = 3\n",
    "# fig, axes = plt.subplots(size, size, figsize=(12, 3 * size))\n",
    "# for i in range(9):\n",
    "#     ax = axes[i // size, i % size]\n",
    "#     ax.imshow(feature_maps[i], cmap='gray')\n",
    "#     ax.axis('off')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_prob_and_labels = sorted(prob_and_labels, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# accuracies_top_k = []\n",
    "# accuracies_less_k = []\n",
    "\n",
    "# for k in range(1, 101):  \n",
    "#     # Top-k Accuracy\n",
    "#     correct_top = sum(1 for prob, label in sorted_prob_and_labels[:k] if label == 1)\n",
    "#     accuracy_top = correct_top / k\n",
    "#     accuracies_top_k.append(accuracy_top)\n",
    "    \n",
    "#     # Less-k Accuracy\n",
    "#     correct_less = sum(1 for prob, label in sorted_prob_and_labels[-k:] if label == 1)\n",
    "#     accuracy_less = correct_less / k\n",
    "#     accuracies_less_k.append(accuracy_less)\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(12, 10))\n",
    "\n",
    "# plt.subplot(2, 1, 1)  \n",
    "# plt.plot(range(1, 101), accuracies_top_k, marker='o', linestyle='-', color='b')\n",
    "# plt.xlabel('Top-k')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Top-k Accuracy')\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.subplot(2, 1, 2)  \n",
    "# plt.plot(range(1, 101), accuracies_less_k, marker='x', linestyle='-', color='r')\n",
    "# plt.xlabel('Less-k')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Less-k (Bottom-k) Accuracy')\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def lg_price_train(features, labels, l1_ratio=0.5, C=1.0):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    features: torch.Tensor\n",
    "    labels: list\n",
    "    l1_ratio: float\n",
    "    C: float \n",
    "    \"\"\"\n",
    "    data_df = features\n",
    "    data_df['label'] = labels\n",
    "    data_df = data_df.dropna()  # Drop rows with any NaNs\n",
    "\n",
    "    X_clean = data_df.drop(columns='label').values\n",
    "    y_clean = data_df['label'].values\n",
    "\n",
    "    model = LogisticRegression(\n",
    "        penalty='elasticnet',\n",
    "        solver='saga',\n",
    "        l1_ratio=l1_ratio,\n",
    "        C=C,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_clean, y_clean)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def lg_image_train(tensor_dataset, l1_ratio=0.5, C=1.0):\n",
    "    \"\"\"\n",
    "    Train a logistic regression model using images from the tensor dataset.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    tensor_dataset: iterable\n",
    "        An iterable of (image, label, ticker) tuples.\n",
    "    l1_ratio: float\n",
    "        The elastic net mixing parameter (0 < l1_ratio < 1).\n",
    "    C: float\n",
    "        Inverse of regularization strength; smaller values specify stronger regularization.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model: LogisticRegression\n",
    "        Trained logistic regression model.\n",
    "    \"\"\"\n",
    "    # Collect features and labels\n",
    "    flattened_features = []\n",
    "    labels = []\n",
    "\n",
    "    for image, label, ticker in tensor_dataset: \n",
    "        flattened_image = image.view(-1)  # Flatten the image tensor\n",
    "        flattened_features.append(flattened_image)\n",
    "\n",
    "        labels.append(label)\n",
    "\n",
    "    # Create a sparse matrix for features\n",
    "    X_sparse = sp.csr_matrix(np.array([f.numpy() for f in flattened_features]))\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler(with_mean=False)  # Set with_mean=False for sparse matrix\n",
    "    X_scaled = scaler.fit_transform(X_sparse)\n",
    "\n",
    "    # Train logistic regression model\n",
    "    model = LogisticRegression(\n",
    "        penalty='elasticnet',\n",
    "        solver='saga',\n",
    "        l1_ratio=l1_ratio,\n",
    "        C=C,\n",
    "        max_iter=1500,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_scaled, labels)\n",
    "\n",
    "    return model\n",
    "\n",
    "def flatten_price_features(df, window=20):\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    df_flattened = (df\n",
    "                    .groupby(df.index // window)\n",
    "                    .apply(lambda x: x.values.flatten())\n",
    "                    .apply(pd.Series))\n",
    "\n",
    "    num_columns = len(df.columns)\n",
    "    df_flattened.columns = [f\"{window - 1 - (i // num_columns)}_{df.columns[i % num_columns]}\"\n",
    "                            for i in range(len(df_flattened.columns))]\n",
    "\n",
    "    df_flattened.index = range(df_flattened.shape[0])\n",
    "\n",
    "    return df_flattened\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = pd.read_pickle(pd.DataFrame(images_dict[0]['2018-10-16_2020-10-16']).iloc[1,3])\n",
    "\n",
    "# temp = flatten_price_features(features, window=20)\n",
    "\n",
    "# labels = np.array(pd.DataFrame(images_dict[0]['2018-10-16_2020-10-16']).iloc[0,3])[:,1].astype(int)\n",
    "\n",
    "# lg_price_model = lg_price_train(temp,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create sector network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xpu目前不支持 稀疏矩陣運算\n",
    "import scipy.sparse as sp\n",
    "import torch.nn.functional as F\n",
    "from pygcn.models import GCN\n",
    "\n",
    "def normalize(mx):\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse_coo_tensor(indices, values, shape, dtype=torch.float32)\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).float()  \n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "class Args:\n",
    "    no_xpu = False\n",
    "    fastmode = False\n",
    "    seed = 42\n",
    "    epochs = 100\n",
    "    lr = 0.01\n",
    "    weight_decay = 5e-4\n",
    "    hidden = 64\n",
    "    dropout = 0.5\n",
    "    \n",
    "temp = df[['Ticker','Sector']].drop_duplicates(subset='Ticker', keep='first')\n",
    "ticker_sector_map = temp.set_index('Ticker')['Sector']\n",
    "sector_array = ticker_sector_map.values\n",
    "adj_matrix = np.equal.outer(sector_array, sector_array).astype(int)\n",
    "\n",
    "# adj = sp.csr_matrix(np.array(adj_matrix)[:,:],dtype = np.float32)\n",
    "# adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "# adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "del temp,ticker_sector_map,sector_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "def gcn_image_train(tensor_dataset, adj, existing_model=None, reset_weights=False):\n",
    "    \"\"\"\n",
    "    GCN 模型訓練函數，支持增量學習\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tensor_dataset : Dataset\n",
    "        訓練數據集\n",
    "    adj : tensor\n",
    "        鄰接矩陣\n",
    "    existing_model : GCN, optional\n",
    "        現有的GCN模型，用於增量學習\n",
    "    reset_weights : bool, optional\n",
    "        是否重置模型權重，默認False\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    GCN : 訓練後的模型\n",
    "    \"\"\"\n",
    "    \n",
    "    # features processing\n",
    "    flattened_features = []\n",
    "    labels = []\n",
    "    \n",
    "    for image, label, ticker in tensor_dataset: \n",
    "        flattened_image = image.view(-1)\n",
    "        flattened_features.append(flattened_image.numpy())  \n",
    "        labels.append(label.numpy()) \n",
    "\n",
    "    flattened_features = np.array(flattened_features)\n",
    "    y = np.array(labels)\n",
    "    y = torch.LongTensor(y).to(device)\n",
    "\n",
    "    flattened_features = sp.csr_matrix(flattened_features, dtype=np.float32)\n",
    "    flattened_features = torch.FloatTensor(flattened_features.todense()).to(device)\n",
    "    \n",
    "    # initial model\n",
    "    args = Args()\n",
    "    args.xpu = not args.no_xpu and torch.xpu.is_available()\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.xpu:\n",
    "        torch.xpu.manual_seed(args.seed)\n",
    "\n",
    "    # 檢查是否使用現有模型\n",
    "    if existing_model is not None and not reset_weights:\n",
    "        gcn_model = existing_model\n",
    "        print(\"Using existing model for incremental learning\")\n",
    "    else:\n",
    "        gcn_model = GCN(nfeat=flattened_features.shape[1],\n",
    "                        nhid=args.hidden,\n",
    "                        nclass=2,\n",
    "                        dropout=args.dropout).to(device)\n",
    "        print(\"Initializing new model\")\n",
    "\n",
    "    # 確保模型在正確的設備上\n",
    "    gcn_model = gcn_model.to(device)\n",
    "    optimizer = optim.Adam(gcn_model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "    # Training function\n",
    "    def train(epoch):\n",
    "        t = time.time()\n",
    "        gcn_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = gcn_model(flattened_features, adj.to(device))\n",
    "        loss_train = F.cross_entropy(output, y)\n",
    "        acc_train = accuracy(output, y)\n",
    "        \n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:  # 每10個epoch打印一次，避免輸出過多\n",
    "            print(f'Epoch: {epoch + 1:04d}, loss_train: {loss_train.item():.4f}, acc_train: {acc_train:.4f}, time: {time.time() - t:.4f}s')\n",
    "        \n",
    "        return loss_train.item(), acc_train\n",
    "\n",
    "    # Training loop\n",
    "    t_total = time.time()\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for epoch in range(args.epochs):\n",
    "        loss, acc = train(epoch)\n",
    "        losses.append(loss)\n",
    "        accuracies.append(acc)\n",
    "        \n",
    "        # # 可以加入早停機制\n",
    "        # if len(losses) > 5 and np.mean(losses[-5:]) > np.mean(losses[-10:-5]):\n",
    "        #     print(\"Early stopping due to increasing loss\")\n",
    "        #     break\n",
    "            \n",
    "    print(\"Optimization Finished!\")\n",
    "    print(f\"Total time elapsed: {time.time() - t_total:.4f}s\")\n",
    "    print(f\"Final training accuracy: {accuracies[-1]:.4f}\")\n",
    "    \n",
    "    return gcn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df = np.array(pd.DataFrame(images_dict[0]).iloc[0, 0][0])\n",
    "image_paths = list(image_df[:,0])\n",
    "labels = list(image_df[:,1].astype(int))\n",
    "tensor_dataset = StockDataset(state='transform', image_paths=image_paths, labels=labels)\n",
    "adj = torch.tensor(adj_matrix, dtype=torch.float32)\n",
    "gcn_model = gcn_image_train(tensor_dataset,adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_cnn_sample(period_images):\n",
    "    label_data = {0: [], 1: []}\n",
    "\n",
    "    for data_list in [period_images[key][0] for key in period_images]:\n",
    "        label_data[0].extend([item for item in data_list if item[1] == 0])\n",
    "        label_data[1].extend([item for item in data_list if item[1] == 1])\n",
    "\n",
    "    max_samples = min(len(label_data[0]), len(label_data[1]), 2500)\n",
    "    sampled_data = random.sample(label_data[0], max_samples) + random.sample(label_data[1], max_samples)\n",
    "    random.shuffle(sampled_data)  \n",
    "\n",
    "    sampled_data = np.array(sampled_data)\n",
    "    image_paths, labels = sampled_data[:, 0], sampled_data[:, 1].astype(int)\n",
    "\n",
    "    #if imlabnce: undersampling\n",
    "    if labels.mean() > 0.8 or labels.mean() < 0.2:\n",
    "        rus = RandomUnderSampler(sampling_strategy=0.6, random_state=42)\n",
    "        df_resampled, labels = rus.fit_resample(pd.DataFrame({'path': image_paths, 'label': labels}), labels)\n",
    "        image_paths = df_resampled['path'].tolist()\n",
    "\n",
    "    train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "        image_paths, labels, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    train_tensor_dataset = StockDataset(state='transform', image_paths=train_paths, labels=train_labels)\n",
    "    test_tensor_dataset = StockDataset(state='transform', image_paths=test_paths, labels=test_labels)\n",
    "    \n",
    "    return train_tensor_dataset, test_tensor_dataset\n",
    "\n",
    "def random_gcn_sample(period_images):\n",
    "    \n",
    "    train_tensors = []\n",
    "    selected_keys = random.sample(list(period_images.keys()), 25)\n",
    "\n",
    "    for key in selected_keys:\n",
    "        image_df = pd.DataFrame(period_images[key][0])\n",
    "        image_paths = list(image_df[0])\n",
    "        labels = list(image_df[1])\n",
    "        train_tensor = StockDataset(state='transform', image_paths=image_paths, labels=labels)\n",
    "    \n",
    "    train_tensors.append(train_tensor)\n",
    "    \n",
    "    return train_tensors\n",
    "\n",
    "def random_lg_price_sample(folder, df):\n",
    "\n",
    "    train = df[df['folder_name'] == folder].copy()\n",
    "    \n",
    "    unique_tickers = df['Ticker'].unique()\n",
    "    \n",
    "\n",
    "    valid_indices = train.index[:-20]\n",
    "    \n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "    target_size = 5000\n",
    "    \n",
    "\n",
    "    cols_to_process = ['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
    "    \n",
    "\n",
    "    def normalize_features(group):\n",
    "        min_vals = group[cols_to_process].min()\n",
    "        max_vals = group[cols_to_process].max()\n",
    "        return (group[cols_to_process] - min_vals) / (max_vals - min_vals)\n",
    "    \n",
    "\n",
    "    grouped_data = {ticker: train[train['Ticker'] == ticker] for ticker in unique_tickers}\n",
    "    \n",
    "    while len(features) < target_size:\n",
    "\n",
    "        batch_size = min(100, target_size - len(features))\n",
    "\n",
    "        random_indices = np.random.randint(0, len(valid_indices), batch_size)\n",
    "        random_tickers = np.random.choice(unique_tickers, batch_size)\n",
    "        \n",
    "        for idx, ticker in zip(random_indices, random_tickers):\n",
    "            ticker_data = grouped_data[ticker]\n",
    "            sample_indices = valid_indices[idx:idx+20]\n",
    "            \n",
    "            sample = ticker_data[ticker_data.index.isin(sample_indices)]\n",
    "            \n",
    "            if len(sample) == 20:\n",
    "\n",
    "                feature = normalize_features(sample)\n",
    "                label = sample['y'].iloc[-1]\n",
    "                \n",
    "                features.append(feature)\n",
    "                labels.append(label)\n",
    "                \n",
    "                if len(features) >= target_size:\n",
    "                    break\n",
    "    features_array = np.array(features) \n",
    "    features_reshaped = features_array.reshape(-1, features_array.shape[-1])  \n",
    "\n",
    "    features = pd.DataFrame(features_reshaped, columns=['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume'])\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['folder_name'] = None\n",
    "\n",
    "for _, window in pd.DataFrame(time_windows).iterrows():\n",
    "\n",
    "    trading_start, trading_end = window['folder_name'].split('_')\n",
    "    trading_start = pd.to_datetime(trading_start)\n",
    "    trading_end = pd.to_datetime(trading_end)\n",
    "\n",
    "    mask = (df.index >= trading_start) & (df.index <= trading_end)\n",
    "    \n",
    "    df.loc[mask, 'folder_name'] = window['folder_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>19_Adj Close</th>\n",
       "      <th>19_Close</th>\n",
       "      <th>19_High</th>\n",
       "      <th>19_Low</th>\n",
       "      <th>19_Open</th>\n",
       "      <th>19_Volume</th>\n",
       "      <th>18_Adj Close</th>\n",
       "      <th>18_Close</th>\n",
       "      <th>18_High</th>\n",
       "      <th>18_Low</th>\n",
       "      <th>...</th>\n",
       "      <th>1_Low</th>\n",
       "      <th>1_Open</th>\n",
       "      <th>1_Volume</th>\n",
       "      <th>0_Adj Close</th>\n",
       "      <th>0_Close</th>\n",
       "      <th>0_High</th>\n",
       "      <th>0_Low</th>\n",
       "      <th>0_Open</th>\n",
       "      <th>0_Volume</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.222223</td>\n",
       "      <td>0.222223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400002</td>\n",
       "      <td>0.375001</td>\n",
       "      <td>0.305273</td>\n",
       "      <td>0.111109</td>\n",
       "      <td>0.111109</td>\n",
       "      <td>0.100002</td>\n",
       "      <td>0.400002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.900002</td>\n",
       "      <td>0.624999</td>\n",
       "      <td>0.350265</td>\n",
       "      <td>0.666664</td>\n",
       "      <td>0.666664</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.624999</td>\n",
       "      <td>0.213409</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.192000</td>\n",
       "      <td>0.168317</td>\n",
       "      <td>0.097345</td>\n",
       "      <td>0.129984</td>\n",
       "      <td>0.049180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712871</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.422949</td>\n",
       "      <td>0.893442</td>\n",
       "      <td>0.887931</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.849557</td>\n",
       "      <td>0.251775</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.721805</td>\n",
       "      <td>0.721805</td>\n",
       "      <td>0.559322</td>\n",
       "      <td>0.620370</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.560896</td>\n",
       "      <td>0.834587</td>\n",
       "      <td>0.834587</td>\n",
       "      <td>0.516949</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.058869</td>\n",
       "      <td>0.278196</td>\n",
       "      <td>0.278196</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037742</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428570</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.293696</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571428</td>\n",
       "      <td>0.444445</td>\n",
       "      <td>0.533334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.124701</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571428</td>\n",
       "      <td>0.444445</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.499999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.678919</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099999</td>\n",
       "      <td>0.453040</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.228104</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0.179273</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.003753</td>\n",
       "      <td>0.195569</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.578948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.785715</td>\n",
       "      <td>0.871380</td>\n",
       "      <td>0.880067</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.456181</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0.433334</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.607142</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.607142</td>\n",
       "      <td>0.209207</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.399999</td>\n",
       "      <td>0.428570</td>\n",
       "      <td>0.470589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.455401</td>\n",
       "      <td>0.199999</td>\n",
       "      <td>0.199999</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.428732</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0.180328</td>\n",
       "      <td>0.180328</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.128686</td>\n",
       "      <td>0.213115</td>\n",
       "      <td>0.213115</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.409836</td>\n",
       "      <td>0.621951</td>\n",
       "      <td>0.622479</td>\n",
       "      <td>0.639344</td>\n",
       "      <td>0.639344</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.672131</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.316221</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0.992366</td>\n",
       "      <td>0.992367</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977612</td>\n",
       "      <td>0.636193</td>\n",
       "      <td>0.908398</td>\n",
       "      <td>0.908397</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060151</td>\n",
       "      <td>0.089553</td>\n",
       "      <td>0.095999</td>\n",
       "      <td>0.030535</td>\n",
       "      <td>0.030534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026192</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108621</td>\n",
       "      <td>0.599997</td>\n",
       "      <td>0.599998</td>\n",
       "      <td>0.399998</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.250001</td>\n",
       "      <td>0.199393</td>\n",
       "      <td>0.858092</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.199997</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      19_Adj Close  19_Close   19_High    19_Low   19_Open  19_Volume  \\\n",
       "0         0.222223  0.222223  0.000000  0.400002  0.375001   0.305273   \n",
       "1         0.000000  0.206897  0.192000  0.168317  0.097345   0.129984   \n",
       "2         0.721805  0.721805  0.559322  0.620370  0.464286   0.560896   \n",
       "3         0.428571  0.428570  0.333333  0.466667  0.375000   0.293696   \n",
       "4         0.875000  0.875000  0.950000  0.954545  0.950000   0.678919   \n",
       "...            ...       ...       ...       ...       ...        ...   \n",
       "4995      0.179273  0.500000  0.409091  0.500001  0.476190   0.003753   \n",
       "4996      0.433334  0.433333  0.607142  0.500000  0.607142   0.209207   \n",
       "4997      0.180328  0.180328  0.112500  0.081967  0.024390   0.128686   \n",
       "4998      0.992366  0.992367  1.000000  1.000000  0.977612   0.636193   \n",
       "4999      0.000000  0.000000  0.000000  0.200001  0.000000   0.108621   \n",
       "\n",
       "      18_Adj Close  18_Close   18_High    18_Low  ...     1_Low    1_Open  \\\n",
       "0         0.111109  0.111109  0.100002  0.400002  ...  0.900002  0.624999   \n",
       "1         0.049180  0.000000  0.000000  0.029703  ...  0.712871  1.000000   \n",
       "2         0.834587  0.834587  0.516949  0.759259  ...  0.000000  0.437500   \n",
       "3         0.571429  0.571428  0.444445  0.533334  ...  0.333333  0.312500   \n",
       "4         1.000000  1.000000  1.000000  0.954545  ...  0.000000  0.099999   \n",
       "...            ...       ...       ...       ...  ...       ...       ...   \n",
       "4995      0.195569  0.520833  0.431818  0.578948  ...  0.815789  0.785715   \n",
       "4996      0.400000  0.399999  0.428570  0.470589  ...  0.205882  0.214286   \n",
       "4997      0.213115  0.213115  0.112500  0.131148  ...  0.409836  0.621951   \n",
       "4998      0.908398  0.908397  1.000000  0.954887  ...  0.060151  0.089553   \n",
       "4999      0.599997  0.599998  0.399998  0.200001  ...  0.200001  0.250001   \n",
       "\n",
       "      1_Volume  0_Adj Close   0_Close    0_High     0_Low    0_Open  0_Volume  \\\n",
       "0     0.350265     0.666664  0.666664  0.500000  1.000000  0.624999  0.213409   \n",
       "1     0.422949     0.893442  0.887931  0.864000  0.940594  0.849557  0.251775   \n",
       "2     0.058869     0.278196  0.278196  0.000000  0.120370  0.000000  0.037742   \n",
       "3     0.124701     0.571429  0.571428  0.444445  0.600000  0.499999  0.000000   \n",
       "4     0.453040     0.166667  0.166667  0.050000  0.136364  0.000000  0.228104   \n",
       "...        ...          ...       ...       ...       ...       ...       ...   \n",
       "4995  0.871380     0.880067  0.854167  0.886364  1.000000  1.000000  0.456181   \n",
       "4996  0.455401     0.199999  0.199999  0.035714  0.205882  0.071429  0.428732   \n",
       "4997  0.622479     0.639344  0.639344  0.525000  0.672131  0.536585  0.316221   \n",
       "4998  0.095999     0.030535  0.030534  0.000000  0.000000  0.000000  0.026192   \n",
       "4999  0.199393     0.858092  0.200001  0.199997  0.200001  0.000000  0.000000   \n",
       "\n",
       "      label  \n",
       "0         1  \n",
       "1         0  \n",
       "2         0  \n",
       "3         1  \n",
       "4         1  \n",
       "...     ...  \n",
       "4995      0  \n",
       "4996      1  \n",
       "4997      0  \n",
       "4998      0  \n",
       "4999      1  \n",
       "\n",
       "[5000 rows x 121 columns]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "features,labels = random_lg_price_sample(folder,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LG Image model saved to C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\training\\models\\2018-10-16_2020-10-16\\lg_price_model.pkl\n",
      "LG Image model saved to C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\training\\models\\2019-01-15_2021-01-15\\lg_price_model.pkl\n",
      "LG Image model saved to C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\training\\models\\2019-04-16_2021-04-16\\lg_price_model.pkl\n",
      "LG Image model saved to C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\training\\models\\2019-07-16_2021-07-16\\lg_price_model.pkl\n",
      "LG Image model saved to C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\training\\models\\2019-10-15_2021-10-15\\lg_price_model.pkl\n",
      "LG Image model saved to C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\training\\models\\2020-01-14_2022-01-14\\lg_price_model.pkl\n",
      "LG Image model saved to C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\training\\models\\2020-04-15_2022-04-15\\lg_price_model.pkl\n",
      "LG Image model saved to C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\training\\models\\2020-07-15_2022-07-15\\lg_price_model.pkl\n",
      "LG Image model saved to C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\training\\models\\2020-10-14_2022-10-14\\lg_price_model.pkl\n",
      "LG Image model saved to C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\training\\models\\2021-01-16_2023-01-16\\lg_price_model.pkl\n",
      "LG Image model saved to C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\training\\models\\2021-04-14_2023-04-14\\lg_price_model.pkl\n",
      "LG Image model saved to C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\training\\models\\2021-07-14_2023-07-14\\lg_price_model.pkl\n",
      "LG Image model saved to C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\training\\models\\2021-10-16_2023-10-16\\lg_price_model.pkl\n",
      "LG Image model saved to C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\training\\models\\2022-01-16_2024-01-16\\lg_price_model.pkl\n",
      "LG Image model saved to C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\training\\models\\2022-04-16_2024-04-16\\lg_price_model.pkl\n",
      "LG Image model saved to C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\training\\models\\2022-07-16_2024-07-16\\lg_price_model.pkl\n",
      "LG Image model saved to C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\training\\models\\2022-10-16_2024-10-16\\lg_price_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "device = torch.device('xpu')\n",
    "base_output_dir = r\"C:\\Users\\USER\\Desktop\\113-1\\GCNs\\CNN_Stock\\training\\models\"\n",
    "\n",
    "# model_paths = {}\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "for folder in pd.DataFrame(time_windows)['folder_name']:\n",
    "\n",
    "    folder_path = os.path.join(base_output_dir, folder)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # model_paths[folder] = {\n",
    "    #     'cnn_model': os.path.join(folder_path, 'cnn_model.pth'),\n",
    "    #     'lg_model': os.path.join(folder_path, 'lg_image_model.pkl'),\n",
    "    #    'gcn_model' = os.path.join(folder_path, 'gcn_model.pth')\n",
    "    # }\n",
    "    model_paths[folder]['lg_price_model'] = os.path.join(folder_path, 'lg_price_model.pkl')\n",
    "\n",
    "\n",
    "    # train_period = images_dict[folder]\n",
    "    # train_tensor_dataset, test_tensor_dataset = random_cnn_sample(train_period)\n",
    "    \n",
    "    # cnn_model = cnn_train(train_tensor_dataset, test_tensor_dataset)\n",
    "    # torch.save(cnn_model.state_dict(), model_paths[folder]['cnn_model'])\n",
    "    # print(f\"CNN model saved to {model_paths[folder]['cnn_model']}\")   \n",
    "\n",
    "\n",
    "    # adj = torch.tensor(adj_matrix, dtype=torch.float32)\n",
    "    # train_tensors = random_gcn_sample(train_period)\n",
    "    # gcn_model = None  # 初始化為 None\n",
    "\n",
    "    # for i, train_tensor in enumerate(train_tensors):\n",
    "    #     print(f\"\\nTraining batch {i+1}/{len(train_tensors)}\")\n",
    "\n",
    "    #     gcn_model = gcn_image_train(\n",
    "    #         tensor_dataset=train_tensor,\n",
    "    #         adj=adj,\n",
    "    #         existing_model=gcn_model,  \n",
    "    #         reset_weights=False  \n",
    "    #     )\n",
    "    \n",
    "    # torch.save(gcn_model, model_paths[folder]['gcn_model'])\n",
    "    \n",
    "    # lg_image_model = lg_image_train(train_tensor_dataset, l1_ratio=0.5, C=1.3)\n",
    "    # joblib.dump(lg_image_model, model_paths[folder]['lg_model'])\n",
    "    # print(f\"LG Image model saved to {model_paths[folder]['lg_model']}\")\n",
    "\n",
    "    temp = flatten_price_features(features, window=20)\n",
    "    features,labels = random_lg_price_sample(folder,df)\n",
    "    lg_price_model = lg_price_train(temp,labels)\n",
    "    joblib.dump(lg_price_model, model_paths[folder]['lg_price_model'])\n",
    "    print(f\"LG Image model saved to {model_paths[folder]['lg_price_model']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ARC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
